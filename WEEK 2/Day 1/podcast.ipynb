{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd5ebd9",
   "metadata": {},
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f45b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory (WEEK 2) to Python path\n",
    "\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "from openai import OpenAI\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display, Markdown\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from src.data_processor import process_input\n",
    "from src.llm_processor import generate_script\n",
    "from src.tts_generator import generate_audio\n",
    "\n",
    "# Load environment variables (API Keys)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create directories for storing recordings\n",
    "os.makedirs('recordings', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f0b6b",
   "metadata": {},
   "source": [
    "Setting Up Open AI API key access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2361e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# The \"../\" tells Python to look one folder up (in the WEEK 1 root)\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Retrieve the key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"✅ API Key successfully loaded!\")\n",
    "else:\n",
    "    print(\"❌ API Key not found. Check your .env file location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51807c",
   "metadata": {},
   "source": [
    "Step 3: Text Pre-processing & Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae396ea",
   "metadata": {},
   "source": [
    "Ensure the text fits within API character limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7820690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 8040 characters from Text input\\supplements_scorecard_notes.txt\n",
      "Processed 3 chunks for TTS.\n",
      "Chunk 1 length: 3949 characters\n",
      "Chunk 2 length: 3629 characters\n",
      "Chunk 3 length: 429 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Load text from Text input folder\n",
    "rec_dir = 'Text input'\n",
    "txts = [f for f in os.listdir(rec_dir) if f.lower().endswith('.txt')]\n",
    "if not txts:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {rec_dir!r}\")\n",
    "\n",
    "fname = os.path.join(rec_dir, txts[0])\n",
    "with open(fname, 'r', encoding='utf-8') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "print(f\"✅ Loaded {len(full_text)} characters from {fname}\")\n",
    "\n",
    "def chunk_text_by_sentences(text, max_chars=4000):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of max_chars, ensuring we don't \n",
    "    break sentences in the middle.\n",
    "    \"\"\"\n",
    "    # 1. Basic cleaning: remove extra whitespace/newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Split text into sentences using regex\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 <= max_chars:\n",
    "            current_chunk += (sentence + \" \")\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunk_text_by_sentences(full_text)\n",
    "\n",
    "print(f\"Processed {len(text_chunks)} chunks for TTS.\")\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Chunk {i+1} length: {len(chunk)} characters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
